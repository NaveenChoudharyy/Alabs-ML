{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83afd346",
   "metadata": {},
   "source": [
    "# Customer Review Analysis For Leading woman clothing E-Commerce Company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e2836",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3f570",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_classif\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from textblob import TextBlob\n",
    "import re as re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import pyLDAvis.gensim_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade openpyxl\n",
    "#!pip install --upgrade bottleneck\n",
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing = pd.read_excel(\"C:/Users/navee/OneDrive/Desktop/Data_Science_360/Case_studies/Completed/Machine_learning/NLP_E_Commerce_Platform_case_study/Womens_Clothing_Reviews_Data.xlsx\")\n",
    "\n",
    "\n",
    "clothing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0510d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496631c",
   "metadata": {},
   "source": [
    "# 1. Performing exploratory analysis on the data to understand the patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff6a99",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data information to understand the data\n",
    "\n",
    "clothing.info()\n",
    "\n",
    "'''\n",
    "\n",
    "Column names in the data  are not following the naming convention. We need to correct this\n",
    "\n",
    "Columns \"Category\", \"SubCategory2\" and \"SubCategory1\" have some missing values. \n",
    "These columns can be dropped for the analysis as we already have huge amount of data.\n",
    "\n",
    "\n",
    "Columns \"Review Title\" and \"Review Text\" also have some missing data.\n",
    "We need to drop those records which have null in both \"Review Title\" and \"Review Text\" columns.\n",
    "Then we need to merge these two columns to create a new column \"Review\"\n",
    "Now we can drop these twor columns (\"Review Title\" and \"Review Text\")\n",
    "\n",
    "We need to drop duplicate records if present\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df010a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dropping duplicate records\n",
    "\n",
    "\n",
    "sum(clothing.duplicated())\n",
    "\n",
    "# No duplicate record found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51335e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Correcting the columns names\n",
    "\n",
    "clothing.columns = clothing.columns.str.replace(' ', '_')\n",
    "\n",
    "clothing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8188449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dropping nulls in the columns \"Category\", \"SubCategory2\" and \"SubCategory1\"\n",
    "\n",
    "\n",
    "clothing.dropna(subset=[\"Category\", \"SubCategory2\", \"Subcategory1\"], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65091e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dropping columns 'Review_Title' and 'Review_Text' where both columns are null.\n",
    "\n",
    "\n",
    "clothing.dropna(subset=['Review_Title', 'Review_Text'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Mearging the data in the columns \"Review_Title' and 'Review_Text\" to create a new column as \"Review\"\n",
    "\n",
    "clothing[\"Review\"] = clothing['Review_Title'].fillna(\"\") + \" \" + clothing['Review_Text'].fillna(\"\")\n",
    "\n",
    "\n",
    "clothing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbed60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dropping columns \"Review_Title' and 'Review_Text\" as now we don't need these\n",
    "\n",
    "\n",
    "clothing.drop(columns=[\"Review_Title\", \"Review_Text\"] , axis=0, inplace=True)\n",
    "\n",
    "\n",
    "clothing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd91aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing.nunique()\n",
    "\n",
    "\n",
    "#### We don't need column \"Product_ID\" as cardinality is very high \n",
    "\n",
    "#### We will treat column \"Customer_Age\" as continous variable as this column have higher cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dropping column \"Product_ID\"\n",
    "\n",
    "\n",
    "clothing.drop(columns=\"Product_ID\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56191efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating two columns \"cust_age_range\" and \"cust_age_category\" to ceate age categories for the custiomers\n",
    "\n",
    "\n",
    "clothing[\"cust_age_range\"] = pd.qcut(clothing.Customer_Age, 10)\n",
    "clothing[\"cust_age_category\"] = pd.qcut(clothing.Customer_Age, 10, labels=range(1,11))\n",
    "\n",
    "\n",
    "clothing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of avilable sub-categories\n",
    "\n",
    "print(\"Total number of sub categories : \", clothing[['Category', \"Subcategory1\", \n",
    "                                                     \"SubCategory2\"]].drop_duplicates()[\"SubCategory2\"].count())\n",
    "\n",
    "\n",
    "clothing[['Category', \"Subcategory1\", \"SubCategory2\"]].sort_values(by=['Category', \"Subcategory1\", \n",
    "                                                                       \"SubCategory2\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ab2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF  \n",
    "def continuous_var_summary( x ):\n",
    "    \n",
    "    # freq and missings\n",
    "    n_total = x.shape[0]\n",
    "    n_miss = x.isna().sum()\n",
    "    perc_miss = n_miss * 100 / n_total\n",
    "    \n",
    "    # outliers - iqr\n",
    "    q1 = x.quantile(0.25)\n",
    "    q3 = x.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lc_iqr = q1 - 1.5 * iqr\n",
    "    uc_iqr = q3 + 1.5 * iqr\n",
    "    \n",
    "    return pd.Series( [ x.dtype, x.nunique(), n_total, x.count(), n_miss, perc_miss,\n",
    "                       x.sum(), x.mean(), x.std(), x.var(), \n",
    "                       lc_iqr, uc_iqr, \n",
    "                       x.min(), x.quantile(0.01), x.quantile(0.05), x.quantile(0.10), \n",
    "                       x.quantile(0.25), x.quantile(0.5), x.quantile(0.75), \n",
    "                       x.quantile(0.90), x.quantile(0.95), x.quantile(0.99), x.max() ], \n",
    "                     \n",
    "                    index = ['dtype', 'cardinality', 'n_tot', 'n', 'nmiss', 'perc_miss',\n",
    "                             'sum', 'mean', 'std', 'var',\n",
    "                        'lc_iqr', 'uc_iqr',\n",
    "                        'min', 'p1', 'p5', 'p10', 'p25', 'p50', 'p75', 'p90', 'p95', 'p99', 'max']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa62afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For continous variable\n",
    "\n",
    "# As we are treating column \"Customer_Age\" as continous variable\n",
    "\n",
    "\n",
    "continuous_var_summary(clothing.loc[:,'Customer_Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For categorical variable\n",
    "\n",
    "clothing.loc[:,['Rating', 'Recommend_Flag']] = clothing.loc[:,['Rating', 'Recommend_Flag']].astype('object')\n",
    "\n",
    "\n",
    "\n",
    "clothing[['Category', 'Subcategory1', 'SubCategory2', 'Location', \n",
    "          'Channel', 'Review', 'Rating', 'Recommend_Flag']].describe()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Some of the reviews are duplicates.\n",
    "\n",
    "Most of the ratings are 5 stars.\n",
    "\n",
    "Website is the most used channel for the shopping.\n",
    "\n",
    "Gurgaon is the location where most of the sales are taking place.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e210f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Duplicate reviews\n",
    "\n",
    "\n",
    "clothing[clothing.duplicated(subset=['Review'], keep=False)].sort_values(by='Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Displaying the Count of 'Recommend_Flag' Values\n",
    "\n",
    "\n",
    "chart_Recommend_Flag = pd.DataFrame(clothing.Recommend_Flag.value_counts()).reset_index()\n",
    "chart_Recommend_Flag.columns = ['Recommend_Flag', 'count_of_Recommend_Flag']\n",
    "\n",
    "chart_Recommend_Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655685c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualization \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.barplot(y=chart_Recommend_Flag.count_of_Recommend_Flag, x=chart_Recommend_Flag.Recommend_Flag, \n",
    "            data=chart_Recommend_Flag, palette='gist_earth', width=0.2)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Recommend Flag\", fontsize=12)\n",
    "plt.ylabel(\"Count of Recommend Flag \", fontsize=12)\n",
    "plt.title(\"Count of Recommend Flag Values\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Displaying the Count of 'Rating' Values\n",
    "\n",
    "\n",
    "chart_Rating = pd.DataFrame(clothing.Rating.value_counts()).reset_index()\n",
    "chart_Rating.columns = ['Rating', 'count_of_Rating' ]\n",
    "chart_Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17074263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.barplot(y=chart_Rating.count_of_Rating, x=chart_Rating.Rating, \n",
    "            data=chart_Rating, palette='gist_earth', width=0.3)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Rating\", fontsize=12)\n",
    "plt.ylabel(\"Count of Rating \", fontsize=12)\n",
    "plt.title(\"Count of Rating Values\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Displaying the Count of 'Channel' Values\n",
    "\n",
    "\n",
    "chart_Channel = pd.DataFrame(clothing.Channel.value_counts()).reset_index()\n",
    "chart_Channel.columns  = ['Channel', 'count_of_Channel']\n",
    "chart_Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa67ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.barplot(y=chart_Channel.count_of_Channel, x=chart_Channel.Channel, \n",
    "            data=chart_Channel, palette='gist_earth', width=0.2)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Channel\", fontsize=12)\n",
    "plt.ylabel(\"Count of Channel \", fontsize=12)\n",
    "plt.title(\"Count of Channel Values\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b98af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Displaying the Count of 'Location' Values\n",
    "\n",
    "\n",
    "chart_Location = pd.DataFrame(clothing.Location.value_counts()).reset_index()\n",
    "chart_Location.columns = ['Location', 'count_of_Location']\n",
    "chart_Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03959d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.barplot(y=chart_Location.count_of_Location, x=chart_Location.Location, \n",
    "            data=chart_Location, palette='gist_earth', width=0.3)\n",
    "\n",
    "\n",
    "plt.xlabel(\"chart_Location\", fontsize=12)\n",
    "plt.ylabel(\"Count of chart_Location \", fontsize=12)\n",
    "plt.title(\"Count of chart_Location Values\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a394bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Proportion of \"Rating\" \n",
    "\n",
    "\n",
    "chart_Rating_proportion = pd.DataFrame(clothing.Rating.value_counts()/clothing.Rating.count()*100).reset_index()\n",
    "chart_Rating_proportion.columns = ['Rating', 'percentage_of_Rating']\n",
    "chart_Rating_proportion\n",
    "\n",
    "# Data is unbalanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.barplot(y=chart_Rating_proportion.percentage_of_Rating, x=chart_Rating_proportion.Rating, \n",
    "            data=chart_Rating_proportion, palette='gist_earth', width=0.3)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Rating\", fontsize=12)\n",
    "plt.ylabel(\"Percentage\", fontsize=12)\n",
    "plt.title(\"Breakdown of Clothing Ratings by Percentage\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fe8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Proportion of Recommend_Flag \n",
    "\n",
    "\n",
    "chart_Recommend_Flag_proportion = pd.DataFrame(clothing.Recommend_Flag.value_counts()/clothing.\n",
    "                                               Recommend_Flag.count()*100).reset_index()\n",
    "chart_Recommend_Flag_proportion.columns = ['Recommend_Flag', 'percentage_of_Recommend_Flag']\n",
    "chart_Recommend_Flag_proportion\n",
    "# Data is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.barplot(y=chart_Recommend_Flag_proportion.percentage_of_Recommend_Flag, x=chart_Recommend_Flag_proportion.Recommend_Flag, \n",
    "            data=chart_Recommend_Flag_proportion, palette='gist_earth', width=0.2)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Recommend Flag\", fontsize=12)\n",
    "plt.ylabel(\"Percentage \", fontsize=12)\n",
    "plt.title(\"Breakdown of Clothing Recommend Flag by Percentage\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf48e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Breakdown of Customer age by Percentage\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.distplot(clothing.Customer_Age, color='r')\n",
    "\n",
    "plt.xlabel(\"Customer Age\", fontsize=12)\n",
    "plt.ylabel(\"Percentage \", fontsize=12)\n",
    "plt.title(\"Breakdown of Customer age by Percentage\", fontsize=15, pad=18)\n",
    "\n",
    "plt.grid(alpha=0.7,linestyle='--', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualization of percentage of count of Recommend_Flag per Rating\n",
    "\n",
    "\n",
    "chart_1 = clothing.groupby(['Rating', 'Recommend_Flag']).agg({'Rating': 'count'}).rename(columns={'Rating':'Rating_count'})\n",
    "\n",
    "\n",
    "chart_1 = chart_1.reset_index().pivot_table(index='Rating', columns='Recommend_Flag', values='Rating_count')\n",
    "\n",
    "\n",
    "chart_1 = chart_1.div(chart_1.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "chart_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1.plot(kind='bar', stacked=True, width=0.4, figsize=(12,5), colormap='Pastel1')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('100% Stacked Column Chart of Recommend_Flag Count by Rating', pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualization of percentage of count of Rating per Recommend_Flag\n",
    "\n",
    "\n",
    "chart_2 = clothing.groupby(['Recommend_Flag', 'Rating']).agg({'Recommend_Flag': 'count'}).rename(columns={'Recommend_Flag':'Recommend_Flag_count'})\n",
    "\n",
    "chart_2 = chart_2.reset_index().pivot_table(values= 'Recommend_Flag_count', columns='Rating', index='Recommend_Flag')\n",
    "\n",
    "chart_2 = chart_2.div(chart_2.sum(axis=1), axis=0)\n",
    "\n",
    "chart_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_2.plot(kind='bar', stacked=True, width=0.3, figsize=(12,5), colormap='Pastel1')\n",
    "plt.xlabel('Recommend_Flag')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('100% Stacked Column Chart of Rating Count by Recommend_Flag', pad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b03f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Relationship between coluumns 'Recommend_Flag' and 'Rating'.\n",
    "\n",
    "heatmap_data = clothing[['Recommend_Flag', \n",
    "                         'Rating']].pivot_table(index='Recommend_Flag', \n",
    "                                                columns='Rating', aggfunc=len, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b044790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Relationship between Rating and Recommend_Flag \n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='Pastel1', fmt='d', linewidths=.9)\n",
    "\n",
    "plt.title('Heatmap: Relationship between Rating and Recommend_Flag', pad=18)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Recommend_Flag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cf016",
   "metadata": {},
   "source": [
    "## Text-EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Only alphabetic word count in each Review:\n",
    "### Considering words like wasn't or it's as one single word (using ' in Regex)\n",
    "\n",
    "clothing.Review.apply( lambda x: [w for w in re.split\n",
    "                                  (r\"[^a-zA-Z']+\", x) \n",
    "                                  if len(w)>1] ).apply( lambda x: len(' '.join(x).split()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bf357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count of alphanumeric words in each Review:\n",
    "\n",
    "\n",
    "clothing['Review'].apply( lambda x: len(re.findall(r\"([a-zA-Z]+[0-9]+|[0-9]+[a-zA-Z]+)\", x))  ).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count of numbers in each Review:\n",
    "\n",
    "\n",
    "clothing['Review'].apply( lambda x: len(re.findall(r\"\\d+\", x))  ).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05772c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count of characters which are neither alphabetic nor numeric (like symbols and whitespaces) in each Review:\n",
    "\n",
    "\n",
    "clothing[\"Review\"].apply( lambda x: len(re.split(r\"\\w+\", x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56325164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Unique alphabetic word count in each review:\n",
    "\n",
    "\n",
    "clothing.Review.apply( lambda x: [w for w in re.split\n",
    "                                  (r\"[^a-zA-Z']+\", x) \n",
    "                                  if len(w)>1] ).apply( lambda x: len(set(' '.join(x).split())) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Characters count in each Review\n",
    "\n",
    "\n",
    "clothing['Review'].apply( lambda x: len(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### punctuation count of each review:\n",
    "\n",
    "\n",
    "clothing['Review'].apply( lambda x: len([w for w in str(x) if w in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stopwords in each review:\n",
    "\n",
    "stop_words = list(set(list(ENGLISH_STOP_WORDS) + list(STOPWORDS)))\n",
    "\n",
    "\n",
    "clothing[\"Review\"].apply( lambda x: [w for w in re.split(r\"[^a-zA-Z']+\", x) if w in stop_words] )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6b3d9e2",
   "metadata": {},
   "source": [
    "#### Adding parts of speech to each word\n",
    "#### Taking too much time to run this\n",
    "\n",
    "\n",
    "clothing.loc[\"Review\"].apply( lambda x: nltk.pos_tag(nltk.word_tokenize(x)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25094116",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28723b",
   "metadata": {},
   "source": [
    "# 2. Perform text mining tasks to understand what most frequent words are using for positive sentiment and negative sentiment. Create word clouds for the positive & negative reviews separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3a98c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a new column naming it as \"polarity\". It contains polarity of the review.\n",
    "### Let if polarity is below 0 or 0, it will be treated as \"Negative_review\"\n",
    "### Let if polarity is above 0, it will be treated as \"Positive_review\"\n",
    "\n",
    "\n",
    "clothing[\"polarity\"] = clothing[\"Review\"].apply( lambda x: TextBlob(str(x)).polarity )\n",
    "\n",
    "\n",
    "clothing[\"polarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing[\"sentiment\"] = np.where(clothing[\"polarity\"] > 0, 'Positive_review', 'Negative_review')\n",
    "\n",
    "clothing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a14216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Seprating positive and negative reviews\n",
    "\n",
    "\n",
    "pos_rev = clothing.loc[clothing[\"sentiment\"] == \"Positive_review\", ['Review', 'sentiment']]\n",
    "\n",
    "neg_rev = clothing.loc[clothing[\"sentiment\"] == \"Negative_review\", ['Review', 'sentiment']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcaf67a4",
   "metadata": {},
   "source": [
    "#### Correcting spelling mistakes\n",
    "#### Not running this code as it is taking almost an hour to run this\n",
    "\n",
    "\n",
    "pos_rev[\"Review\"] = pos_rev[\"Review\"].apply( lambda x: str(TextBlob(x).correct()) )\n",
    "\n",
    "neg_rev[\"Review\"] = neg_rev[\"Review\"].apply( lambda x: str(TextBlob(x).correct()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ea641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenization\n",
    "\n",
    "\n",
    "pos_rev[\"Review\"] = pos_rev[\"Review\"].apply( lambda x: [x for x in re.split\n",
    "                                                        (r\"[^a-zA-Z']+\", x) if len(x) > 1] )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "neg_rev[\"Review\"] = neg_rev[\"Review\"].apply( lambda x: [x for x in re.split\n",
    "                                                        (r\"[^a-zA-Z']+\", x) if len(x) > 1] )\n",
    "\n",
    "\n",
    "\n",
    "print(pos_rev[\"Review\"])\n",
    "print(\" \")\n",
    "print(neg_rev[\"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c50e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lemmatization (to find root words)\n",
    "\n",
    "\n",
    "pos_rev[\"Review\"] = pos_rev[\"Review\"].apply( lambda x: [WordNetLemmatizer().lemmatize(w) for w in x] )\n",
    "\n",
    "\n",
    "neg_rev[\"Review\"] = neg_rev[\"Review\"].apply( lambda x: [WordNetLemmatizer().lemmatize(w) for w in x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Joing Joining the words back to create a scentance\n",
    "\n",
    "\n",
    "pos_rev[\"Review\"] = pos_rev[\"Review\"].apply( lambda x: ' '.join(x) )\n",
    "\n",
    "\n",
    "neg_rev[\"Review\"] = neg_rev[\"Review\"].apply( lambda x: ' '.join(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a95772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Vectorization\n",
    "\n",
    "\n",
    "vectorization = CountVectorizer(stop_words=(list(set(list(ENGLISH_STOP_WORDS) + list(STOPWORDS) +[\"wa\", \"dd\", \"ha\", \"ve\", \"ll\", \"tt\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting the data\n",
    "\n",
    "vectorization_pos = vectorization.fit(pos_rev[\"Review\"])\n",
    "\n",
    "\n",
    "vectorization_neg = vectorization.fit(neg_rev[\"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_col = list(vectorization_pos.get_feature_names_out())\n",
    "neg_col = list(vectorization_neg.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2797298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"neg_col : \")\n",
    "print(\" \")\n",
    "print(neg_col)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"pos_col : \")\n",
    "print(\" \")\n",
    "print(pos_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8225d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Transforming the data\n",
    "\n",
    "\n",
    "vectorization_pos = vectorization_pos.transform(pos_rev[\"Review\"]).todense()\n",
    "vectorization_neg = vectorization_neg.transform(neg_rev[\"Review\"]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c12651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding the frequency of meaningful words other then stopwords.\n",
    "### Extracting only top 300 meaningful words  \n",
    "\n",
    "\n",
    "pos_word_freq = pd.DataFrame(pd.DataFrame(\n",
    "    vectorization_pos, columns=pos_col).sum(axis=0), \n",
    "                             columns=['freq']).reset_index().sort_values(by='freq', \n",
    "                                                                      ascending=False).head(300)\n",
    "\n",
    "\n",
    "\n",
    "neg_word_freq = pd.DataFrame(pd.DataFrame(\n",
    "    vectorization_neg, columns=neg_col).sum(axis=0), \n",
    "                             columns=['freq']).reset_index().sort_values(by='freq', \n",
    "                                                                      ascending=False).head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Correcting column names\n",
    "\n",
    "\n",
    "pos_word_freq.columns = ['word', 'freq']\n",
    "neg_word_freq.columns = ['word', 'freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508fa96",
   "metadata": {},
   "source": [
    "#### Most frequent words for positive reviews are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(pos_word_freq.word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959c89d",
   "metadata": {},
   "source": [
    "#### Most frequent words for negative reviews are :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9647ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(neg_word_freq.word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547365a",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b503bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating list of positive words and their frequencies\n",
    "\n",
    "\n",
    "words_pos = list(pos_word_freq.word)\n",
    "freq_pos = list(pos_word_freq.freq)\n",
    "\n",
    "\n",
    "\n",
    "#### Creating list of negative words and their frequencies\n",
    "\n",
    "\n",
    "words_neg = list(neg_word_freq.word)\n",
    "freq_neg = list(neg_word_freq.freq)\n",
    "\n",
    "\n",
    "\n",
    "print(\"words_pos : \", words_pos)\n",
    "print(\" \")\n",
    "print(\"freq_pos : \", freq_pos)\n",
    "print(\" \")\n",
    "print(\"words_neg : \", words_neg)\n",
    "print(\" \")\n",
    "print(\"freq_neg : \", freq_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dictionary using \"words_pos\" as \"key\" and \"freq_pos\" as its value.\n",
    "\n",
    "pos_dict = dict(zip(words_pos, freq_pos))\n",
    "\n",
    "\n",
    "\n",
    "#### Creating dictionary using \"words_neg\" as \"key\" and \"freq_neg\" as its value.\n",
    "\n",
    "neg_dict = dict(zip(words_neg, freq_neg))\n",
    "\n",
    "\n",
    "\n",
    "print(\"pos_dict : \", pos_dict)\n",
    "print(\" \")\n",
    "print(\"neg_dict :\", neg_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generateing a word cloud for positive words\n",
    "\n",
    "pos_word_cloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(pos_dict)\n",
    "\n",
    "\n",
    "# Generateing a word cloud for negative words\n",
    "\n",
    "neg_word_cloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(neg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e8c52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(pos_word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34918db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(neg_word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcff79",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99c3fb",
   "metadata": {},
   "source": [
    "###### As we can clearly see in the above two word clouds, there are some words which are present in both word clouds. It is very difficult to understand which words in the review make the polarity of the review negative and which make it positive. To overcome this problem, we can identify those words that are present in either of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dictionary of positive words which are present in \"pos_dict\" but not in \"neg_dict\"\n",
    "\n",
    "only_pos_dict = {key: value for key, value in pos_dict.items() if key not in neg_dict}\n",
    "\n",
    "\n",
    "\n",
    "#### Creating dictionary of negative words which are present in \"neg_dict\" but not in \"pos_dict\"\n",
    "\n",
    "only_neg_dict = {key: value for key, value in neg_dict.items() if key not in pos_dict}\n",
    "\n",
    "\n",
    "\n",
    "print(\"only_pos_dict : \", only_pos_dict)\n",
    "print(\" \")\n",
    "print(\"only_neg_dict : \", only_neg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f64e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrating a word cloud for \"only_pos_dict\"\n",
    "\n",
    "wordcloud_only_pos_dict = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(only_pos_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Genrating a word cloud for \"only_pos_dict\"\n",
    "\n",
    "wordcloud_only_neg_dict = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(only_neg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(wordcloud_only_pos_dict)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12296f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(wordcloud_only_neg_dict)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589902",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Relationship between coluumns 'Recommend_Flag' and 'sentiment'.\n",
    "\n",
    "heatmap_data_2 = clothing[['Recommend_Flag', \n",
    "                         'sentiment']].pivot_table(index='Recommend_Flag', \n",
    "                                                columns='sentiment', aggfunc=len, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Relationship between sentiment and Recommend_Flag \n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(heatmap_data_2, annot=True, cmap='Pastel1', fmt='d', linewidths=.9)\n",
    "\n",
    "plt.title('Heatmap: Relationship between sentiment and Recommend_Flag', pad=18)\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('Recommend_Flag')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#### With the help of this chart we ccan understand that even if sometimes review is positive but still it will not\n",
    "#### get recommended and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Relationship between coluumns 'sentiment' and 'Rating'.\n",
    "\n",
    "heatmap_data_3 = clothing[['Rating', \n",
    "                         'sentiment']].pivot_table(index='Rating', \n",
    "                                                columns='sentiment', aggfunc=len, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Relationship between Rating and sentiment \n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(heatmap_data_3, annot=True, cmap='Pastel1', fmt='d', linewidths=.9)\n",
    "\n",
    "plt.title('Heatmap: Relationship between sentiment and Rating', pad=18)\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#### With the help of this chart we ccan understand that even if sometimes review is positive but still it will not\n",
    "#### get recommended and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23e7d8",
   "metadata": {},
   "source": [
    "# 3. Understand sentiment among the customers on the different categories, sub categories, products by location and age group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d0dec",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec711718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sentiment by Category\n",
    "\n",
    "\n",
    "clothing.groupby(['Category', 'sentiment'])[['sentiment']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65344805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sentiment by Subcategory1\n",
    "\n",
    "\n",
    "clothing.groupby(['Subcategory1', 'sentiment'])[['sentiment']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sentiment by SubCategory2\n",
    "\n",
    "\n",
    "clothing.groupby(['SubCategory2', 'sentiment'])[['sentiment']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd91ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sentiment by location\n",
    "\n",
    "\n",
    "clothing.groupby(['Location', 'sentiment'])[['sentiment']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906ca95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Sentiment by Age category\n",
    "\n",
    "\n",
    "clothing.groupby(['cust_age_category', 'sentiment'])[['sentiment']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fae04",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00a24f",
   "metadata": {},
   "source": [
    "# 4. Perform predictive analytics to understand the drivers of customers who are recommending the products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d33a5",
   "metadata": {},
   "source": [
    "# Binomial Classification of Recommend_Flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924ba44",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To understand the customers behavior about recommending the products we need to perform classification modelling.\n",
    "\n",
    "### In this case \"Recommend_Flag\" will be the y-variable. \n",
    "### As per above analysis Column 'Customer_Age' will be continous variable because this column has high cardinality.\n",
    "### As per above analysis Columns 'Category', 'Subcategory1', 'SubCategory2', 'Location', 'Channel', 'Rating', 'Review' will \\\n",
    "#   categorical x-variables as these have low cardinality and column 'Review' is contain strings. \n",
    "### We need to clean the data in the column \"Review\" and need to vectorize the keywords.\n",
    "\n",
    "\n",
    "####** For this analysis i will not be using \"derived columns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4bbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a new DataFrame with only required columns\n",
    "\n",
    "\n",
    "df = clothing[['Category', 'Subcategory1', 'SubCategory2', 'Location', 'Channel','Customer_Age', 'Rating', 'Recommend_Flag', 'Review']]\n",
    "\n",
    "df = df.reset_index().drop(columns='index')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Getting information about the new data\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc62f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As \"Customer_Age\" is continous variable we need to perform outlier treatment if required.\n",
    "### Choosing IQR method for outlier treatment as range is not too big\n",
    "\n",
    "\n",
    "q1 = df.Customer_Age.quantile(0.25)\n",
    "q3 = df.Customer_Age.quantile(0.75)\n",
    "iqr = q3-q1\n",
    "\n",
    "\n",
    "upper_range = q3 + 1.5*iqr\n",
    "lower_range = q1 - 1.5*iqr\n",
    "\n",
    "print('upper_range : ', upper_range)\n",
    "print('lower_range : ', lower_range)\n",
    "print('iqr : ', iqr)\n",
    "print('Customer max age : ', df.Customer_Age.max())\n",
    "print('Customer min age : ', df.Customer_Age.min())\n",
    "print(\"\")\n",
    "print('Number of records having customer age greater then upper_range : ', df.loc[df.Customer_Age >= upper_range].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Treating outliers\n",
    "\n",
    "df['Customer_Age'] = df['Customer_Age'].apply( lambda x: lower_range if x < lower_range else upper_range if x> upper_range else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dummies for all categorical x-variables except column \"Review\"\n",
    "\n",
    "#### It is giving True and False so converting it into 0 and 1 using .astype(int) function.\n",
    "\n",
    "dummies_1 = pd.get_dummies(data=df[['Category', 'Subcategory1', 'SubCategory2', 'Location', \n",
    "                                    'Channel', 'Rating']], drop_first=True).astype(int)\n",
    "\n",
    "dummies_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging column \"Review\", \"Customer_Age\" and Recommend_Flag\n",
    "\n",
    "df = pd.concat(objs=[dummies_1, df[[\"Customer_Age\", \"Review\", \"Recommend_Flag\"]]], axis=1)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb697c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if data is imbalanced\n",
    "\n",
    "df.Recommend_Flag.value_counts()/df.Recommend_Flag.shape[0]\n",
    "\n",
    "\n",
    "#### Data is imbalanced so splitting the data into train test and then sampling the data \n",
    "#### to avoid data leakage and data duplicacy in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train test split\n",
    "\n",
    "train, test = train_test_split(df, train_size=0.7, random_state=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57056c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train_data EDA\n",
    "\n",
    "train.Recommend_Flag.value_counts()/train.Recommend_Flag.shape[0]\n",
    "\n",
    "# Data is still imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd02582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making data balanced\n",
    "\n",
    "# Using Under sampling technique for this\n",
    "### Using astype(int) as y-variable should only be continous variable for classification\n",
    "\n",
    "x_resampled, y_resampled = RandomOverSampler(random_state=1).fit_resample(train.iloc[:, :-1], train.iloc[:, -1].astype(int))\n",
    "\n",
    "\n",
    "print(x_resampled.shape, y_resampled.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Checking the balance of the data again to confirm\n",
    "\n",
    "y_resampled.value_counts()/y_resampled.shape[0]\n",
    "\n",
    "# Data is still imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a41a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data cleaning in dataset \"x_resampled\" in the column \"Review\"\n",
    "#### We need to perform TextBlob.correct(), Lemmatization, and \"vectorization (TfidfVectorizer)\"\n",
    "\n",
    "#### Removing alphanumeric, numbers and non-alphanumeric charaters and also removing all alphabets which are single\n",
    "#### Tokenization (word-wise) -- this is important for next step \"Lemmatization\"\n",
    "#### Lemmatization to find the root words\n",
    "#### Now we need to join back all words for this next step\n",
    "#### We need to correct the spelling mistakes\n",
    "\n",
    "\n",
    "def text_clean(y):\n",
    "    y = y.apply( lambda x: x.lower() )\n",
    "    y = y.apply( lambda x: [w for w in re.split(r\"[^a-zA-Z']+\", x) if len(w) > 1])\n",
    "    y = y.apply( lambda x: [WordNetLemmatizer().lemmatize(w) for w in x] )\n",
    "    y = y.apply( lambda x: ' '.join(x) )\n",
    "#    y = y.apply( lambda x: str(TextBlob(x).correct()) )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Text data cleaning for \"x_resampled\" data (\"train data\")\n",
    "\n",
    "x_resampled[\"Review\"] = text_clean(x_resampled[\"Review\"])\n",
    "\n",
    "\n",
    "\n",
    "#---Text data cleaning for \"test\" data \n",
    "\n",
    "\n",
    "test[\"Review\"] = text_clean(test[\"Review\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "459564f7",
   "metadata": {},
   "source": [
    "def vectorization(y):\n",
    "    vect = TfidfVectorizer(stop_words=stop_words+[\"ve\", \"ll\", \"isn\", \"don\", \"doesn\", \"didn\", \"tt\", \"ha\", \"wa\"], min_df=150, max_df=0.99, max_features=300, ngram_range=(1,5))\n",
    "    vect = vect.fit(y)\n",
    "    vect_cols = vect.get_feature_names_out()\n",
    "    vect_matrix = vect.transform(y).todense()\n",
    "    dummies = pd.DataFrame(vect_matrix, columns=vect_cols)\n",
    "    return dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Vectorization\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=stop_words+[\"ve\", \"ll\", \"isn\", \"don\", \"doesn\", \"didn\", \"tt\", \"ha\", \"wa\"], min_df=150, max_df=0.99, max_features=300, ngram_range=(1,5))\n",
    "\n",
    "vect = vect.fit(x_resampled['Review'])\n",
    "\n",
    "vect_cols = vect.get_feature_names_out()\n",
    "\n",
    "vect_matrix = vect.transform(x_resampled['Review']).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a090b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dummies_train = pd.DataFrame(vect_matrix, columns=vect_cols)\n",
    "\n",
    "text_dummies_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538dc027",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dummies_test = pd.DataFrame(vect.transform(test[\"Review\"]).todense(), columns=vect_cols)\n",
    "\n",
    "text_dummies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Concating the above vectorised data and train data\n",
    "\n",
    "\n",
    "train_data = pd.concat([x_resampled.iloc[:, :-1].reset_index().drop(columns='index'), text_dummies_train.reset_index().drop(columns='index')], axis=1)\n",
    "\n",
    "\n",
    "#### Concating the above vectorised transformed data and test data\n",
    "\n",
    "\n",
    "test_data = pd.concat([test.iloc[:, :-2].reset_index().drop(columns='index'), text_dummies_test.reset_index().drop(columns='index')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Correcting column names\n",
    "\n",
    "\n",
    "train_data.columns = train_data.columns.str.replace(\"'\", \"_\").str.replace(\" \", \"_\")\n",
    "train_data.columns = train_data.columns.str.replace(\"1\", \"_a_a\").str.replace(\"2\", \"_b_b\").str.replace(\"3\", \"_c_c\").str.replace(\"4\", \"_d_d\").str.replace(\"5\", \"_e_e\")\n",
    "print(list(train_data.columns))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "test_data.columns = test_data.columns.str.replace(\"'\", \"_\").str.replace(\" \", \"_\")\n",
    "test_data.columns = test_data.columns.str.replace(\"1\", \"_a_a\").str.replace(\"2\", \"_b_b\").str.replace(\"3\", \"_c_c\").str.replace(\"4\", \"_d_d\").str.replace(\"5\", \"_e_e\")\n",
    "\n",
    "print(list(test_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704a38a",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using \"SelectKBest\"\n",
    "\n",
    "\n",
    "skb_feat = SelectKBest(score_func=f_classif, k=20)\n",
    "skb_feat = skb_feat.fit(train_data, y_resampled)\n",
    "\n",
    "\n",
    "skb_list = list((train_data.columns)[skb_feat.get_support()])\n",
    "\n",
    "print(skb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using RFE\n",
    "\n",
    "\n",
    "rfe_feat = RFE(estimator=LogisticRegression(), n_features_to_select=25)\n",
    "rfe_feat = rfe_feat.fit(train_data, y_resampled)\n",
    "\n",
    "rfe_list = list((train_data.columns)[rfe_feat.get_support()])\n",
    "\n",
    "print(rfe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceed04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable selected after combining above two\n",
    "\n",
    "var_selected = list(set(rfe_list + skb_list))\n",
    "\n",
    "print(var_selected)\n",
    "print(len(var_selected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be02c7",
   "metadata": {},
   "source": [
    "###### Removing multi-colinerity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b74d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating formulae to be used in dmatrices\n",
    "\n",
    "\n",
    "f_like = 'Recommend_Flag ~ ' + ' + '.join(var_selected)\n",
    "\n",
    "f_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a formula we need y and x variables in same dataframe\n",
    "# creating a temprory dataframe for this by combining \"y_resampled\" and \"train_data\"\n",
    "\n",
    "\n",
    "tem_df = pd.concat([y_resampled.reset_index().drop(columns='index'), train_data.loc[:, var_selected].reset_index().drop(columns = 'index')], axis = 1)\n",
    "\n",
    "tem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a blank dataframe named as vif\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "\n",
    "#### Designing matrices from above created formulae (f_like)\n",
    "\n",
    "\n",
    "y, x = dmatrices(formula_like=f_like, data=tem_df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b01465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a new column with column names\n",
    "\n",
    "vif['features'] = x.columns\n",
    "\n",
    "\n",
    "#### Creating a new column having vales as variance_inflation_factor \n",
    "\n",
    "vif['vif_factor'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4d85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7353c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_selected = list(vif.loc[vif.vif_factor <= 5, 'features'])\n",
    "\n",
    "print(var_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standardizing the data\n",
    "\n",
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting and transforming the trained data\n",
    "\n",
    "train_transformed = std.fit_transform(train_data.loc[:,var_selected])\n",
    "\n",
    "\n",
    "#### Transforming the test data\n",
    "\n",
    "test_transformed =  std.transform(test_data.loc[:, var_selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d011746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating the dataframe for both trained and test data with appropiate column names\n",
    "\n",
    "\n",
    "x_train = pd.DataFrame(data=train_transformed, columns=train_data.loc[:,var_selected].columns)\n",
    "x_test = pd.DataFrame(data=test_transformed, columns=test_data.loc[:,var_selected].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ff7ea",
   "metadata": {},
   "source": [
    "# Model 1: Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating param grid for Logistics Regression\n",
    "\n",
    "param_grid_lr = {'penalty' : ['l1', 'l2'], 'C': [0.1, 0.01, 0.5, 0.05, 0.25, 0.75]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "LR = GridSearchCV(LogisticRegression(),param_grid_lr, verbose=True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38639d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "\n",
    "\n",
    "LR.fit(x_train, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220dd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "\n",
    "df_LR_train = pd.DataFrame({'Actual': y_resampled, 'Predicted': LR.predict(x_train)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "\n",
    "df_LR_test = pd.DataFrame({'Actual': test.Recommend_Flag.astype(int), 'Predicted': LR.predict(x_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65654f",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_LR_train.Actual, df_LR_train.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85927f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_LR_test.Actual, df_LR_test.Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb89cf",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- Accuracy in both train and test data is same and high which is 94%.\n",
    "###### -- For train data precision rate of class \"0\" is very high then test data.\n",
    "###### -- Higher precision in both train and test data for class \"1\" indicates when model predicts 1 it is likely to be 1.\n",
    "###### -- Recall value for both train and test data is higher and which indicates proportion of actual positive cases that are correctly identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b909e20",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398720d",
   "metadata": {},
   "source": [
    "## Model 2: RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating param grid for RandomForest\n",
    "\n",
    "param_grid_rf = {'n_estimators' : [100,200,300], 'max_leaf_nodes' : [4,7,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322caa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "\n",
    "rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, verbose=True, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6beaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "\n",
    "\n",
    "rf.fit(x_train, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15dba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding best parameters of RandomForest\n",
    "\n",
    "rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "df_rf_train = pd.DataFrame({'Actual': y_resampled, 'Predicted': rf.predict(x_train)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "df_rf_test = pd.DataFrame({'Actual': test.Recommend_Flag.astype(int), 'Predicted': rf.predict(x_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce865b",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_rf_train.Actual, df_rf_train.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18318a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_rf_test.Actual, df_rf_test.Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f499b",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- Accuracy in both train and test data is same and high which is 94%. It is same as Logistics Regression.\n",
    "###### -- For train data precision rate of class \"0\" is very high then test data.\n",
    "###### -- Higher precision in both train and test data for class \"1\" indicates when model predicts 1 it is likely to be 1.\n",
    "###### -- Recall value for both train and test data is higher and which indicates proportion of actual positive cases that are correctly identified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5595a8",
   "metadata": {},
   "source": [
    "## Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b922ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating param grid for XGBoost\n",
    "\n",
    "param_grid_xg = {'learning_rate' : [0.01, 0.05, 0.1], 'n_estimators' : [100,200,300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "xg = GridSearchCV(XGBClassifier(),param_grid_xg, verbose=True, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "\n",
    "xg.fit(x_train, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding best parameters of XGBoost\n",
    "\n",
    "xg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "df_xg_train = pd.DataFrame({'Actual': y_resampled, 'Predicted': xg.predict(x_train)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "df_xg_test = pd.DataFrame({'Actual': test.Recommend_Flag.astype(int), 'Predicted': xg.predict(x_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38bc22",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_xg_train.Actual, df_xg_train.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ab7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_xg_test.Actual, df_xg_test.Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f07d50",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- Accuracy in train data (0.96) is higher then test data (0.93). \n",
    "###### -- For train data precision rate of class \"0\" is very high then test data.\n",
    "###### -- Higher precision in both train and test data for class \"1\" indicates when model predicts 1 it is likely to be 1.\n",
    "###### -- Recall value in train data for class \"0\" is higher then test data.\n",
    "###### -- Recall value in train data for class \"1\" is almost similar test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851a330",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "raw",
   "id": "433e96f8",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2fd542c",
   "metadata": {},
   "source": [
    "# Took too much time to run, not using this."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2b07828",
   "metadata": {},
   "source": [
    "#### Creating param grid for SVM\n",
    "\n",
    "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'class_weight': [None, 'balanced']}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d138efb7",
   "metadata": {},
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "svm_model = grid_search = GridSearchCV(SVC(probability=True, random_state=42), param_grid_svm, cv=5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bb7f1fc",
   "metadata": {},
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "\n",
    "svm_model.fit(x_train, y_resampled)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96f89d82",
   "metadata": {},
   "source": [
    "#### Finding best parameters of XGBoost\n",
    "\n",
    "svm_model.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e6bb867",
   "metadata": {},
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "df_svm_train = pd.DataFrame({'Actual': y_resampled, 'Predicted': svm_model.predict(x_train)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "df_svm_test = pd.DataFrame({'Actual': test.Recommend_Flag.astype(int), 'Predicted': svm_model.predict(x_test)})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "324a830d",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c3da252",
   "metadata": {},
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_svm_train.Actual, df_svm_train.Predicted))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92989a06",
   "metadata": {},
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_svm_test.Actual, df_svm_test.Predicted))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fe16ef5",
   "metadata": {},
   "source": [
    "'''\n",
    "Comments\n",
    "'''\n",
    "#### This model took almost 12 hrs to run. Not using this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d749307",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8cfc8",
   "metadata": {},
   "source": [
    "## All the above models have almost similar results, I would choose Random forest for final model as it is having high accuracy as welll as higher recall and precision values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052410a",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345d877",
   "metadata": {},
   "source": [
    "# Multinomial classification of rating based on Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f23cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We need to clean the data in the column \"Review\" and need to vectorize the keywords.\n",
    "\n",
    "####** For this analysis i will not be using \"derived columns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To understand the customers behavior about Rating the products we need to perform classification modelling.\n",
    "\n",
    "### In this case \"Rating\" will be the y-variable. \n",
    "### As per above analysis Column 'Customer_Age' will be continous variable because this column has high cardinality.\n",
    "### As per above analysis Columns 'Category', 'Subcategory1', 'SubCategory2', 'Location', 'Channel', 'Review' will \n",
    "###   categorical x-variables as these have low cardinality and column 'Review' is contain strings. \n",
    "### We need to clean the data in the column \"Review\" and need to vectorize the keywords.\n",
    "\n",
    "\n",
    "####** For this analysis i will not be using \"derived columns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417aa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a new DataFrame with only required columns\n",
    "\n",
    "\n",
    "data = clothing[['Category', 'Subcategory1', 'SubCategory2', 'Location', 'Channel','Customer_Age', 'Rating', 'Review']]\n",
    "\n",
    "data = data.reset_index().drop(columns='index')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Getting information about the new data\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### As \"Customer_Age\" is continous variable we need to perform outlier treatment if required.\n",
    "### Choosing IQR method for outlier treatment as range is not too big\n",
    "\n",
    "\n",
    "q1_m = data.Customer_Age.quantile(0.25)\n",
    "q3_m = data.Customer_Age.quantile(0.75)\n",
    "iqr_m = q3_m-q1_m\n",
    "\n",
    "\n",
    "upper_range_m = q3_m + 1.5*iqr_m\n",
    "lower_range_m = q1_m - 1.5*iqr_m\n",
    "\n",
    "print('upper_range : ', upper_range_m)\n",
    "print('lower_range : ', lower_range_m)\n",
    "print('iqr : ', iqr_m)\n",
    "print('Customer max age : ', data.Customer_Age.max())\n",
    "print('Customer min age : ', data.Customer_Age.min())\n",
    "print(\"\")\n",
    "print('Number of records having customer age greater then upper_range : ', data.loc[data.Customer_Age >= upper_range_m].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Treating outliers\n",
    "\n",
    "data['Customer_Age'] = data['Customer_Age'].apply( lambda x: lower_range_m if x < lower_range_m else upper_range_m if x> upper_range_m else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dummies for all categorical x-variables except column \"Review\"\n",
    "\n",
    "#### It is giving True and False so converting it into 0 and 1 using .astype(int) function.\n",
    "\n",
    "dummies_1_m = pd.get_dummies(data=data[['Category', 'Subcategory1', 'SubCategory2', 'Location', \n",
    "                                    'Channel']], drop_first=True).astype(int)\n",
    "\n",
    "dummies_1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging column \"Review\", \"Customer_Age\" and Rating\n",
    "\n",
    "data = pd.concat(objs=[dummies_1_m, data[[\"Customer_Age\", \"Review\", \"Rating\"]]], axis=1)\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064253d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if data is imbalanced\n",
    "\n",
    "data.Rating.value_counts()/data.Rating.shape[0]\n",
    "\n",
    "\n",
    "#### Data is imbalanced so splitting the data into train test and then sampling the data \n",
    "#### to avoid data leakage and data duplicacy in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76464304",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train test split\n",
    "\n",
    "train_m, test_m = train_test_split(data, train_size=0.7, random_state=1)\n",
    "\n",
    "print(train_m.shape)\n",
    "print(test_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train_data EDA\n",
    "\n",
    "train_m.Rating.value_counts()/train_m.Rating.shape[0]\n",
    "\n",
    "# Data is still imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a63643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making data balanced\n",
    "\n",
    "# Using Under sampling technique for this\n",
    "### Using astype(int) as y-variable should only be continous variable for classification\n",
    "\n",
    "x_resampled_m, y_resampled_m = RandomOverSampler(random_state=1).fit_resample(train_m.iloc[:, :-1], train_m.iloc[:, -1].astype(int))\n",
    "\n",
    "\n",
    "print(x_resampled_m.shape, y_resampled_m.shape)\n",
    "print(test_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Checking the balance of the data again to confirm\n",
    "\n",
    "y_resampled_m.value_counts()/y_resampled_m.shape[0]\n",
    "\n",
    "# Data is still imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acda849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data cleaning in dataset \"x_resampled\" in the column \"Review\"\n",
    "#### We need to perform TextBlob.correct(), Lemmatization, and \"vectorization (TfidfVectorizer)\"\n",
    "\n",
    "#### Removing alphanumeric, numbers and non-alphanumeric charaters and also removing all alphabets which are single\n",
    "#### Tokenization (word-wise) -- this is important for next step \"Lemmatization\"\n",
    "#### Lemmatization to find the root words\n",
    "#### Now we need to join back all words for this next step\n",
    "#### We need to correct the spelling mistakes\n",
    "\n",
    "\n",
    "def text_clean(y):\n",
    "    y = y.apply( lambda x: x.lower() )\n",
    "    y = y.apply( lambda x: [w for w in re.split(r\"[^a-zA-Z']+\", x) if len(w) > 1])\n",
    "    y = y.apply( lambda x: [WordNetLemmatizer().lemmatize(w) for w in x] )\n",
    "    y = y.apply( lambda x: ' '.join(x) )\n",
    "#    y = y.apply( lambda x: str(TextBlob(x).correct()) )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633901b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Text data cleaning for \"x_resampled\" data (\"train data\")\n",
    "\n",
    "x_resampled_m[\"Review\"] = text_clean(x_resampled_m[\"Review\"])\n",
    "\n",
    "\n",
    "\n",
    "#---Text data cleaning for \"test\" data \n",
    "\n",
    "\n",
    "test_m[\"Review\"] = text_clean(test_m[\"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Vectorization\n",
    "\n",
    "\n",
    "vect_m = TfidfVectorizer(stop_words=stop_words+[\"ve\", \"ll\", \"isn\", \"don\", \"doesn\", \"didn\", \"tt\", \"ha\", \"wa\"], min_df=150, max_df=0.99, max_features=300, ngram_range=(1,5))\n",
    "\n",
    "vect_m = vect_m.fit(x_resampled_m['Review'])\n",
    "\n",
    "vect_cols_m = vect_m.get_feature_names_out()\n",
    "\n",
    "vect_matrix_m = vect_m.transform(x_resampled_m['Review']).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dummies_train_m = pd.DataFrame(vect_matrix_m, columns=vect_cols_m)\n",
    "\n",
    "text_dummies_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dummies_test_m = pd.DataFrame(vect_m.transform(test_m[\"Review\"]).todense(), columns=vect_cols_m)\n",
    "\n",
    "text_dummies_test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Concating the above vectorised data and train data\n",
    "\n",
    "\n",
    "train_data_m = pd.concat([x_resampled_m.iloc[:, :-1].reset_index().drop(columns='index'), text_dummies_train_m.reset_index().drop(columns='index')], axis=1)\n",
    "\n",
    "\n",
    "#### Concating the above vectorised transformed data and test data\n",
    "\n",
    "\n",
    "test_data_m = pd.concat([test_m.iloc[:, :-2].reset_index().drop(columns='index'), text_dummies_test_m.reset_index().drop(columns='index')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717716dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Correcting column names\n",
    "\n",
    "\n",
    "train_data_m.columns = train_data_m.columns.str.replace(\"'\", \"_\").str.replace(\" \", \"_\")\n",
    "train_data_m.columns = train_data_m.columns.str.replace(\"1\", \"_a_a\").str.replace(\"2\", \"_b_b\").str.replace(\"3\", \"_c_c\").str.replace(\"4\", \"_d_d\").str.replace(\"5\", \"_e_e\")\n",
    "print(list(train_data_m.columns))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "test_data_m.columns = test_data_m.columns.str.replace(\"'\", \"_\").str.replace(\" \", \"_\")\n",
    "test_data_m.columns = test_data_m.columns.str.replace(\"1\", \"_a_a\").str.replace(\"2\", \"_b_b\").str.replace(\"3\", \"_c_c\").str.replace(\"4\", \"_d_d\").str.replace(\"5\", \"_e_e\")\n",
    "\n",
    "print(list(test_data_m.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f836a4",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67676f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using \"SelectKBest\"\n",
    "\n",
    "\n",
    "skb_feat_m = SelectKBest(score_func=f_classif, k=331)\n",
    "skb_feat_m = skb_feat_m.fit(train_data_m, y_resampled_m)\n",
    "\n",
    "\n",
    "var_selected_m = list((train_data_m.columns)[skb_feat_m.get_support()])\n",
    "\n",
    "print(var_selected_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858e1ee",
   "metadata": {},
   "source": [
    "##### Removing multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Correalation between variables in absolute values.\n",
    "\n",
    "multi_c = train_data_m.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a dataframe containing indexes as variable names and values representing number of vaiable it is collinear with.\n",
    "\n",
    "\n",
    "multi_c = pd.DataFrame(((multi_c >0.5).sum() > 1).astype(int), columns= ['t_f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a list having only those variables which have higher multi-collinearity\n",
    "\n",
    "\n",
    "multi_c_list = list(multi_c[multi_c.t_f == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variables to drop from var_selected_m to remove multicollinearity\", multi_c_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dropping variables having multi-collinearity\n",
    "\n",
    "\n",
    "var_selected_m = list(set(var_selected_m) - set(multi_c_list))\n",
    "\n",
    "print(var_selected_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b61d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Standardizing the data\n",
    "\n",
    "std_m = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting and transforming the trained data\n",
    "\n",
    "train_transformed_m = std_m.fit_transform(train_data_m.loc[:,var_selected_m])\n",
    "\n",
    "\n",
    "#### Transforming the test data\n",
    "\n",
    "test_transformed_m =  std_m.transform(test_data_m.loc[:, var_selected_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating the dataframe for both trained and test data with appropiate column names\n",
    "\n",
    "\n",
    "x_train_m = pd.DataFrame(data=train_transformed_m, columns=train_data_m.loc[:,var_selected_m].columns)\n",
    "x_test_m = pd.DataFrame(data=test_transformed_m, columns=test_data_m.loc[:,var_selected_m].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e00a7c",
   "metadata": {},
   "source": [
    "# Model 1: Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9146fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating param grid for Logistics Regression\n",
    "\n",
    "param_grid_lr_m = {'penalty' : ['l1', 'l2'], 'C': [0.1, 0.01, 0.5, 0.05, 0.25, 0.75]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "LR_m = GridSearchCV(LogisticRegression(),param_grid_lr_m, verbose=True, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "\n",
    "\n",
    "LR_m.fit(x_train_m, y_resampled_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d276fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "\n",
    "df_LR_train_m = pd.DataFrame({'Actual': y_resampled_m, 'Predicted': LR_m.predict(x_train_m)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "\n",
    "df_LR_test_m = pd.DataFrame({'Actual': test_m.Rating.astype(int), 'Predicted': LR_m.predict(x_test_m)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9f1e6",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_LR_train_m.Actual, df_LR_train_m.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_LR_test_m.Actual, df_LR_test_m.Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea89d34",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- Accuracy in both train data (0.56) adn test data (0.56) is same. \n",
    "###### -- Precision for all calsses have huge gap in both train and test data.\n",
    "###### -- Recall value in train data for class \"5\" is higher for both train (0.70) and test (0.68) data.\n",
    "###### -- f1-score in train data for class \"5\" is higher for both train (0.68) and test (0.75) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33b279",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f026a5c",
   "metadata": {},
   "source": [
    "## Model 2: RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a80e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating param grid for RandomForest\n",
    "\n",
    "param_grid_rf_m = {'n_estimators' : [100,200,300], 'max_leaf_nodes' : [4,7,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "\n",
    "rf_m = GridSearchCV(RandomForestClassifier(), param_grid_rf_m, verbose=True, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4068536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "\n",
    "\n",
    "rf_m.fit(x_train_m, y_resampled_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77608168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding best parameters of RandomForest\n",
    "\n",
    "rf_m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "df_rf_train_m = pd.DataFrame({'Actual': y_resampled_m, 'Predicted': rf_m.predict(x_train_m)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "df_rf_test_m = pd.DataFrame({'Actual': test_m.Rating.astype(int), 'Predicted': rf_m.predict(x_test_m)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18289f0f",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_rf_train_m.Actual, df_rf_train_m.Predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8550dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_rf_test_m.Actual, df_rf_test_m.Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0158f",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- Model is under-fitted as accuracy in train data (0.42) is lower the test data(0.51).\n",
    "###### -- Precision is lower for all calsses in both train and test data.\n",
    "###### -- Recall value in train data for class \"5\" is higher for both train (0.70) and test (0.68) data.\n",
    "###### -- f1-score in train data for class \"5\" is higher for both train (0.68) and test (0.75) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10779fef",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757313d",
   "metadata": {},
   "source": [
    "## Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bbac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating param grid for XGBoost\n",
    "\n",
    "param_grid_xg_m = {'learning_rate' : [0.01, 0.05, 0.1], 'n_estimators' : [100,200,300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting parameters in GridSearchCV\n",
    "\n",
    "xg_m = GridSearchCV(XGBClassifier(),param_grid_xg_m, verbose=True, cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9660622",
   "metadata": {},
   "source": [
    "###### In XGBoost model y-variable values starts with 0 so subtracting all with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb80db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Fitting the data in trained data for x-variables and trained data for y_variables\n",
    "y_resampled_m_a = y_resampled_m.apply(lambda x: x-1)\n",
    "xg_m.fit(x_train_m, y_resampled_m_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding best parameters of XGBoost\n",
    "\n",
    "xg_m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating dataframe having actual y-variable of trained data and predicted y-variable of trained data\n",
    "\n",
    "df_xg_train_m = pd.DataFrame({'Actual': y_resampled_m_a, 'Predicted': xg_m.predict(x_train_m)})\n",
    "\n",
    "\n",
    "#### Creating dataframe having actual y-variable of test data and predicted y-variable of test data\n",
    "\n",
    "test_m_Rating_m_a = test_m.Rating.astype(int).apply(lambda x: x-1)\n",
    "df_xg_test_m = pd.DataFrame({'Actual': test_m_Rating_m_a, 'Predicted': xg_m.predict(x_test_m)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fd9f7",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Train data\n",
    "\n",
    "print(classification_report(df_xg_train_m.Actual+1, df_xg_train_m.Predicted+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2799a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report of Test data\n",
    "\n",
    "print(classification_report(df_xg_test_m.Actual+1, df_xg_test_m.Predicted+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3946dd",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- Model is overfitted as accuracy in train data (0.91) is very higher then the test data(0.58).\n",
    "###### -- Precision for class 5 is higher in both train and test data but for all other classes precision is low.\n",
    "###### -- Recall value for call calsses is lower in test data then the train data.\n",
    "###### -- f1-score for call calsses is lower in test data then the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057ff55",
   "metadata": {},
   "source": [
    "## Logistics Regression is better compared all models. \n",
    "\n",
    "###### After creating multiple models we can say \"Logistics Regression\" is the best model among all 3 in terms of accuracy\n",
    "###### but for recall, f1-score and precision none of the models are good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad483dc",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872eb879",
   "metadata": {},
   "source": [
    "# 5. Create topics and understand themes behind the topics by performing topic mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bc9f9",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bfc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Review = clothing.reset_index().drop(columns='index')\n",
    "Review = Review.Review\n",
    "Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cleaning the column \"Review\" in the dataframe \"clothing\" and storing as in \"Reviews\" as series.\n",
    "\n",
    "Reviews = text_clean(Review)\n",
    "Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a128e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stopwords to remove from this series \"Reviews\"\n",
    "\n",
    "stop_words = stop_words+[\"ve\", \"ll\", \"isn\", \"don\", \"doesn\", \"didn\", \"tt\", \"ha\", \"wa\"]\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing the stop words from \"Reviews\" by using list comprehension\n",
    "\n",
    "Reviews = Reviews.apply( lambda x: [w for w in x.split() if w not in stop_words] )\n",
    "\n",
    "\n",
    "print(Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Don't want single character words\n",
    "\n",
    "Reviews = Reviews.apply(lambda x: [w for w in x if len(w) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating \"corpora.Dictionary\" for \"Reviews\" (list of words)\n",
    "\n",
    "\n",
    "Reviews_dict = corpora.Dictionary(Reviews)\n",
    "\n",
    "print(Reviews_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fa129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Converting Reviews_dict into document term matrix\n",
    "\n",
    "\n",
    "doc_term_matrix = [Reviews_dict.doc2bow(w) for w in Reviews]\n",
    "\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fb3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating an object for LDA model using gensim library\n",
    "\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a for loop to find the best value of \"number of topics\" \n",
    "\n",
    "for i in range(2,15):\n",
    "    LDA = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = LDA(doc_term_matrix, num_topics=i, id2word=Reviews_dict, passes=1, minimum_probability=0)\n",
    "    print(i,\" : \",ldamodel.log_perplexity(doc_term_matrix))\n",
    "    \n",
    "    \n",
    "    \n",
    "'''Lower the log_perplexity, better the model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abff97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Running the LDA model on doc_term_matrix with num_topics = 14 as log_perplexity for 14 topics is highest (-7.450).\n",
    "\n",
    "%time ldamodel = LDA(doc_term_matrix, num_topics=14, id2word=Reviews_dict, passes=14, minimum_probability=0, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9081eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, Reviews_dict, sort_topics=False, mds='mmds')\n",
    "#pyLDAvis.display(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the above model, clusters are overlapping with each other (in visualization)\n",
    "## So, This model is not best fit for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time lets try with num_topics = 13\n",
    "# log_perplexity = -7.402\n",
    "\n",
    "%time ldamodel = LDA(doc_term_matrix, num_topics=13, id2word=Reviews_dict, passes=14, minimum_probability=0, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, Reviews_dict, sort_topics=False, mds='mmds')\n",
    "#pyLDAvis.display(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b76d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the above model also, clusters are overlapping with each other (in visualization)\n",
    "## So, This model is not best fit for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time lets try with num_topics = 12\n",
    "# log_perplexity = -7.347\n",
    "\n",
    "%time ldamodel = LDA(doc_term_matrix, num_topics=12, id2word=Reviews_dict, passes=14, minimum_probability=0, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, Reviews_dict, sort_topics=False, mds='mmds')\n",
    "#pyLDAvis.display(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the above model also, clusters are overlapping with each other (in visualization)\n",
    "## So, This model is not best fit for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time lets try with num_topics = 11\n",
    "# log_perplexity = -7.347\n",
    "\n",
    "%time ldamodel = LDA(doc_term_matrix, num_topics=11, id2word=Reviews_dict, passes=14, minimum_probability=0, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7925b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "pyLDAvis.gensim_models.prepare(ldamodel, doc_term_matrix, Reviews_dict, sort_topics=False, mds='mmds')\n",
    "#pyLDAvis.display(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8285c1",
   "metadata": {},
   "source": [
    "###### This model is final as there is no overlapping among clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"log_perplexity : \", ldamodel.log_perplexity(doc_term_matrix))\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "for i in range(0,11):\n",
    "    print(\"Topic : \", i)\n",
    "    print(ldamodel.print_topic(i, topn=10))\n",
    "    print(\"__________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Printing topics with weightage of words\n",
    "\n",
    "ldamodel.print_topics(num_topics = 11, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extracting topics from above excluding weightage\n",
    "\n",
    "\n",
    "def extracting_topics(weighted_tuples_list):\n",
    "    all_sentences = []\n",
    "\n",
    "    for _, topic_string in weighted_tuples_list:\n",
    "        words_list = re.findall(r'\"([^\"]*)\"', topic_string)\n",
    "        sentence = ' '.join(words_list)\n",
    "        all_sentences.append(sentence)\n",
    "\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd651f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Extracting topics from above excluding weightage\n",
    "\n",
    "\n",
    "topics = pd.Series(extracting_topics(ldamodel.print_topics(num_topics = 20, num_words=20)))\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff97bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in ldamodel[doc_term_matrix]:\n",
    "    print( \"doc : \", count, i)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf82b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamodel, corpus=doc_term_matrix, texts=Reviews):\n",
    "    # Init output\n",
    "    rows = []\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                row_data = [int(topic_num), round(prop_topic, 4), topic_keywords, texts[i]]\n",
    "                rows.append(row_data)\n",
    "                break\n",
    "\n",
    "    sent_topics_df = pd.DataFrame(rows, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Original_Text'])\n",
    "    return sent_topics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Themes = format_topics_sentences(ldamodel=ldamodel, corpus=doc_term_matrix, texts=Reviews)\n",
    "Themes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba8202",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
